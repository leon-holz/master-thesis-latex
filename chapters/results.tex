\chapter{Results}


\section{Encoder Models}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different encoder models. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
e5 Large Instruct & 0.1154 & 0.1462 & 0.1680 & 0.4062 & 369,834 \\
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Kalm Mini Instruct & 0.1158 & 0.1465 & 0.1636 & 0.4011 & 369,834 \\
Bilingual & 0.1165 & 0.1473 & 0.1559 & 0.3912 & 369,834 \\
Gwen & 0.1180 & 0.1490 & 0.1361 & 0.3658 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Encoders_r2.png}
    \caption{Popularity prediction results for popularity regressor with different encoder models}
    \label{fig:boxplot_encoder}
\end{figure}

\section{Prompts}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different encoder prompts. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
Stella Popularity & 0.1150 & 0.1456 & 0.1738 & 0.4134 & 369,834 \\
Stella Writing Quality & 0.1148 & 0.1454 & 0.1759 & 0.4158 & 369,834 \\
Stella Engagement & 0.1152 & 0.1459 & 0.1708 & 0.4099 & 369,834 \\
Stella Baseline & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Stella Political Leaning & 0.1151 & 0.1456 & 0.1729 & 0.4122 & 369,834 \\
Stella Content & 0.1148 & 0.1454 & 0.1762 & 0.4164 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Prompts_r2.png}
    \caption{Popularity prediction results for popularity regressor with different encoder prompts}
    \label{fig:boxplot_prompt}
\end{figure}

\section{Inputs}

\subsection{Dimensionality Reduction}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different dimensionality reductions. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Per-Subreddit PCA 512 & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Global PCA 256 & 0.1156 & 0.1462 & 0.1669 & 0.4049 & 369,834 \\
Global PCA 128 & 0.1154 & 0.1461 & 0.1680 & 0.4064 & 369,834 \\
Global PCA 512 & 0.1207 & 0.1519 & 0.0976 & 0.2630 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Dimensionality_Reduction_r2.png}
    \caption{Popularity prediction results for popularity regressor with different dimensionality reductions}
    \label{fig:boxplot_dimensionality}
\end{figure}



%show different dim reductions evals
all dimensionality reductions decrease performance
it seems the 1024 all contain relevant information, some of which would be lost by reducing them
we therefore use full 1024

\subsection{Aggregation methods}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different aggregation methods. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
Hadamard Product & 0.1211 & 0.1525 & 0.0918 & 0.2843 & 369,834 \\
Mean & 0.1150 & 0.1457 & 0.1731 & 0.4124 & 369,834 \\
Multi-Feature & 0.1153 & 0.1459 & 0.1707 & 0.4094 & 369,834 \\
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Min Pooling & 0.1154 & 0.1460 & 0.1691 & 0.4078 & 369,834 \\
Max Pooling & 0.1154 & 0.1461 & 0.1683 & 0.4066 & 369,834 \\
Smart Mean & 0.1152 & 0.1458 & 0.1717 & 0.4107 & 369,834 \\
Title Only & 0.1173 & 0.1483 & 0.1447 & 0.3763 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Aggregation_Methods_r2.png}
    \caption{Popularity prediction results for popularity regressor with }
    \label{fig:boxplot_aggregation_methods}
\end{figure}

%show different aggregation method evaluation
despite being the most naive approach, concat has the best performance by a small margin
this aligns with our previous finding that the information in the 1024 is relevant and any method that that tries to makethe information more compact decreases performance

\subsection{Link vs Self Posts}

the next thing we tried was to use separate models for self posts and link posts




\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Post_Hint_Separation_r2.png}
    \caption{Popularity prediction results for popularity regressor with Post Hint separation}
    \label{fig:boxplot_post_hint}
\end{figure}



\subsection{Additional Inputs}

only use inputs that are available at time of prediction and that can be leveraged for own post creation (no comment amount, no karma or other author information)

top terms coverage tfidf
domain analysis

\subsubsection{Domain Analysis}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different author feature combinations. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
Self+Link - Domain All & 0.1146 & 0.1452 & 0.1792 & 0.4200 & 369,834 \\
Self+Link - Domain Scores & 0.1146 & 0.1452 & 0.1793 & 0.4201 & 369,834 \\
Self+Link - Domain Binary & 0.1153 & 0.1459 & 0.1709 & 0.4097 & 369,834 \\
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Domain_Analysis_r2.png}
    \caption{Popularity prediction results for popularity regressor with domain features}
    \label{fig:boxplot_domain}
\end{figure}


\subsubsection{Post Time Features}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different author feature combinations. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
Post Timestamp & 0.1151 & 0.1458 & 0.1722 & 0.4115 & 369,834 \\
Hour of Day & 0.1152 & 0.1459 & 0.1709 & 0.4097 & 369,834 \\
Month of Year & 0.1152 & 0.1458 & 0.1713 & 0.4104 & 369,834 \\
Time Buckets & 0.1152 & 0.1458 & 0.1712 & 0.4099 & 369,834 \\
Day of Week & 0.1153 & 0.1459 & 0.1701 & 0.4089 & 369,834 \\
Holidays Minor & 0.1154 & 0.1460 & 0.1697 & 0.4082 & 369,834 \\
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Holidays All & 0.1154 & 0.1460 & 0.1697 & 0.4081 & 369,834 \\
Holidays Binary & 0.1153 & 0.1459 & 0.1698 & 0.4084 & 369,834 \\
Holidays Major & 0.1153 & 0.1459 & 0.1699 & 0.4085 & 369,834 \\
Weekend & 0.1153 & 0.1459 & 0.1699 & 0.4085 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Post_Time_r2.png}
    \caption{Popularity prediction results for popularity regressor with post time features}
    \label{fig:boxplot_post_time}
\end{figure}


\subsubsection{Author Features}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different author feature combinations. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & Precision & Recall & F1-Macro & Accuracy & Total & \% Popular \\
\midrule
Author All & 0.6952 & 0.6354 & 0.6636 & 0.6478 & 220,855 & 50.01\% \\
Author Age & 0.6938 & 0.6268 & 0.6582 & 0.6399 & 220,855 & 50.01\% \\
Author Link Karma & 0.6858 & 0.6227 & 0.6524 & 0.6349 & 220,855 & 50.01\% \\
Author Comment Karma & 0.6839 & 0.6234 & 0.6519 & 0.6350 & 220,855 & 50.01\% \\
Stella & 0.6841 & 0.6223 & 0.6513 & 0.6341 & 369,834 & 50.00\% \\
Author Total Karma & 0.6838 & 0.6210 & 0.6505 & 0.6327 & 220,855 & 50.01\% \\
Author Moderator Status & 0.6838 & 0.6182 & 0.6489 & 0.6304 & 220,855 & 50.01\% \\
Author Email Verified & 0.6827 & 0.6172 & 0.6479 & 0.6293 & 220,855 & 50.01\% \\
Author Gold Status & 0.6823 & 0.6169 & 0.6476 & 0.6289 & 220,855 & 50.01\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different author feature combinations. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
Author All & 0.1090 & 0.1390 & 0.2016 & 0.4444 & 220,855 \\
Author Age & 0.1108 & 0.1412 & 0.1782 & 0.4180 & 220,855 \\
Author Link Karma & 0.1113 & 0.1415 & 0.1737 & 0.4128 & 220,855 \\
Author Comment Karma & 0.1113 & 0.1414 & 0.1748 & 0.4141 & 220,855 \\
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Author Total Karma & 0.1112 & 0.1413 & 0.1758 & 0.4152 & 220,855 \\
Author Moderator Status & 0.1126 & 0.1429 & 0.1576 & 0.3931 & 220,855 \\
Author Email Verified & 0.1125 & 0.1428 & 0.1594 & 0.3955 & 220,855 \\
Author Gold Status & 0.1126 & 0.1429 & 0.1573 & 0.3928 & 220,855 \\
\bottomrule
\end{tabular}
\end{table}

While some author features showed weaker individual performance, the combined feature set achieved optimal results (R²: 0.2016), suggesting synergistic effects between features. Further ablation of specific feature subsets was deemed unnecessary given computational constraints and the marginal nature of potential gains.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Author_Data_r2.png}
    \caption{Popularity prediction results for popularity regressor with author features}
    \label{fig:boxplot_author}
\end{figure}

\subsubsection{TFIDF}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different TFIDF feature combinations. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
TF-IDF All Features & 0.1153 & 0.1460 & 0.1695 & 0.4081 & 369,834 \\
TF-IDF Binary & 0.1153 & 0.1460 & 0.1697 & 0.4081 & 369,834 \\
TF-IDF Scores & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
TF-IDF Coverage & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_TF-IDF_r2.png}
    \caption{Popularity prediction results for popularity regressor with tfidf features}
    \label{fig:boxplot_tfidf}
\end{figure}

\subsubsection{Sentiment}

\begin{table}[ht]
\centering
\caption{Popularity prediction results for popularity regressor with different TFIDF feature combinations. Results show mean performance across all subreddits.}
\label{tab:author_comparison}
\begin{tabular}{lrrrrrr}
\toprule
Features & MAE & RMSE & R² & r & Total \\
\midrule
Separate - Scores & 0.1152 & 0.1459 & 0.1709 & 0.4101 & 369,834 \\
Separate - All Features & 0.1152 & 0.1459 & 0.1710 & 0.4101 & 369,834 \\
Combined - Binary & 0.1153 & 0.1459 & 0.1702 & 0.4089 & 369,834 \\
Separate - Binary & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Combined - Categorical & 0.1153 & 0.1459 & 0.1702 & 0.4088 & 369,834 \\
Max Pooled - Categorical & 0.1153 & 0.1459 & 0.1700 & 0.4087 & 369,834 \\
Max Pooled - Binary & 0.1153 & 0.1459 & 0.1700 & 0.4085 & 369,834 \\
Stella & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Separate - Categorical & 0.1153 & 0.1459 & 0.1700 & 0.4086 & 369,834 \\
Max Pooled - All Features & 0.1153 & 0.1459 & 0.1708 & 0.4097 & 369,834 \\
Max Pooled - Scores & 0.1153 & 0.1459 & 0.1707 & 0.4096 & 369,834 \\
Combined - All Features & 0.1153 & 0.1459 & 0.1700 & 0.4088 & 369,834 \\
Combined - Scores & 0.1153 & 0.1459 & 0.1700 & 0.4087 & 369,834 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/boxplot/boxplot_Sentiment_r2.png}
    \caption{Popularity prediction results for popularity regressor with sentiment features}
    \label{fig:boxplot_sentiment}
\end{figure}

\subsection{Cross-Generalizability}

% todo


\section{Gen AI}

As a baseline, we calculated the average upvote ratio, score and comments per post for all subreddits in our popular post creation test.
we also grouped the posts by users, as a few users might skew the average by creating lots of very popular or unpopular posts. The resulting difference was negligibly though.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{figs/average_post.png}
    \caption{Average Post Metrics per subreddit}
    \label{fig:boxplot_tfidf}
\end{figure}

\begin{table}[ht]
\centering
\caption{Subreddit performance statistics showing median and mean values for engagement metrics across selected communities.}
\label{tab:subreddit_stats}
\begin{tabular}{lrrrrrrr}
\toprule
Subreddit & \multicolumn{2}{c}{Upvote Ratio} & \multicolumn{2}{c}{Score} & \multicolumn{2}{c}{Comments} & Posts \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & Median & Mean & Median & Mean & Median & Mean & Total \\
\midrule
progun & 0.89 & 0.841 & 34 & 118.9 & 10 & 27.3 & 28,649 \\
TrueAtheism & 0.83 & 0.783 & 23 & 61.8 & 26 & 48.7 & 10,127 \\
anarchism & 0.83 & 0.810 & 11 & 39.7 & 4 & 12.6 & 79,539 \\
TrueChristian & 0.82 & 0.790 & 6 & 18.6 & 13 & 24.1 & 46,608 \\
mensrights & 0.82 & 0.788 & 18 & 78.9 & 8 & 22.2 & 140,714 \\
DebateReligion & 0.71 & 0.679 & 6 & 16.1 & 67 & 105.1 & 30,428 \\
PoliticalDiscussion & 0.68 & 0.657 & 5 & 49.9 & 30 & 101.2 & 41,924 \\
\bottomrule
\end{tabular}
\end{table}

